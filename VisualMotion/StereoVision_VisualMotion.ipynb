{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DepthEstimation](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/Questions_IMG/Question1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Solution](http://elvers.us/perception/aperture/)\n",
    "\n",
    "## Take the partial derivative of D\n",
    "\n",
    "$\\nabla D = (\\delta f) B/d + (\\delta B) f/d + -(\\delta d)fB (1/d)^2$\n",
    "\n",
    "$\\nabla D = (\\delta f) B/d + (\\delta B) f/d + -(\\delta d)\\frac{Z^2}{fd}$\n",
    "\n",
    "**Baseline** when $(\\delta f) = (\\delta d) = 0$\n",
    "\n",
    "• $\\delta D = (\\delta B) \\frac{f}{d}$\n",
    "\n",
    "• $\\delta D \\displaystyle\\propto \\delta B $\n",
    "\n",
    "• $\\delta D \\displaystyle\\propto  \\frac{1}{d} $\n",
    "\n",
    "• $\\delta D \\displaystyle\\propto f $\n",
    "\n",
    "**focal length** when $(\\delta B) = (\\delta d) = 0$\n",
    "\n",
    "• $\\delta D = (\\delta f) \\frac{B}{d}$\n",
    "\n",
    "• $\\delta D \\displaystyle\\propto \\delta f $\n",
    "\n",
    "• $\\delta D \\displaystyle\\propto  \\frac{1}{d} $\n",
    "\n",
    "• $\\delta D \\displaystyle\\propto B $\n",
    "\n",
    "**disparity** when $(\\delta f) = (\\delta B) = 0$\n",
    "\n",
    "• $\\delta D = -(\\delta d)\\frac{Z^2}{fd}$\n",
    "\n",
    "• $\\delta D \\displaystyle\\propto \\delta d $\n",
    "\n",
    "• $\\delta D \\displaystyle\\propto  Z^2 $\n",
    "\n",
    "• $\\delta D \\displaystyle\\propto \\frac{1}{f} $\n",
    "\n",
    "• $\\delta D \\displaystyle\\propto \\frac{1}{d} $\n",
    "\n",
    "**disparity** when $(\\delta D) =  0$\n",
    "\n",
    "• There is no error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Question2](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/Questions_IMG/Question2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Solution](https://www.pyimagesearch.com/2015/01/19/find-distance-camera-objectmarker-using-python-opencv/)\n",
    "\n",
    "No, pure rotation only leads to 2D information. You can produce (panoramic) \"mosaicing\" effect by rotation. For pure Translation (T<sub>x</sub>,T<sub>y</sub>,T<sub>z</sub>) consider the case where the camera is moving only in a plane (T<sub>z</sub> = 0) and the focal length remains the same–a Parallel Motion field (not a Radial Motion field). You are able to solve for the depth by estimating the relative change in the **Z** direction of corresponding points. For the 2D information you can solve by substituting for v<sub>x</sub> =  T<sub>z</sub>/Z (x -x<sub>0</sub>) and v<sub>y</sub> =  T<sub>z</sub>/Z (y -y<sub>0</sub>). Multiple images are taken as the camera moves.\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "           v_{x} \\\\\n",
    "           v_{y}\n",
    "         \\end{bmatrix}\n",
    "         = \\frac{\\textbf{1}}{\\textbf{Z}} \n",
    "         \\begin{bmatrix}\n",
    "           -f & 0 & x \\\\\n",
    "           0 & -f & y\n",
    "         \\end{bmatrix}\n",
    "         \\begin{bmatrix}\n",
    "           T_{x} \\\\\n",
    "           T_{y} \\\\\n",
    "           T_{z}\n",
    "         \\end{bmatrix}\n",
    "         +\n",
    "         \\frac{\\textbf{1}}{\\textbf{f}} \n",
    "         \\begin{bmatrix}\n",
    "           xy & −(x^2 + f^2) & fy\\\\\n",
    "           (x^2 + f^2) & -xy & -fx\n",
    "         \\end{bmatrix}\n",
    "         \\begin{bmatrix}\n",
    "           w_{x} \\\\\n",
    "           w_{y} \\\\\n",
    "           w_{z}\n",
    "         \\end{bmatrix}\n",
    "         $$\n",
    "         \n",
    "For correspondence, the assumption is taken at different times, t, in matching the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Question3](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/Questions_IMG/Question3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Solution](http://elvers.us/perception/aperture/)\n",
    "\n",
    "The aperture problem occurs in the optical flow equation when you are estimating the relative motion between the camera and the scene of image point. For a small window view, a point generates two unknowns in its 2D movement. The aperture problem occurs when you can only detect only 1D movement of a point relative to the motion of the camera accurately. For each point you have two unknowns. You need at least two points that are on the same surface to accurately estimate 2D motion relative to the camera. Assume densely packed points, so that the multiple points belong to the same surface. \n",
    "\n",
    "If a corner is visible through the aperture, you have a constraint. Use the vector relationship on either side of the vertex of that corner and solve for the direction of motion perpendicular to an edge of the aperture. The angle in between the corner can be used to correct your 2D estimation between 2 images taken at different times of a moved point. (This is easier if you find a corner that is 90º i.e. edges orthogonal to one another!)\n",
    "\n",
    "In general:\n",
    "\n",
    "• only the component of the motion field in the direction of the spatial image gradient can be determined.\n",
    "\n",
    "• the component perpendicular to the spatial gradient is not constrained by the optical flow equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Question4](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/Questions_IMG/Question4.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Solution](http://ccvcl.org/professor-zhigang-zhu/computer-vision-fall-2020/csc-i6716-fall-2020-assignment-4/)\n",
    "\n",
    "## Fundamental Matrix\n",
    "\n",
    "\n",
    "\n",
    "#### Fundamental Matrix\n",
    "For 8 points: \n",
    "**<center>F = M<sub>r</sub> <sup>T</sup>EM<sub>r</sub> <sup>-1</sup></center>**\n",
    "\n",
    "|column 1 | column 2  | column 3|\n",
    "------------|---------|---------|\n",
    "|-0.0000 |  -0.0000  | -0.0001|\n",
    "|   0.0000 |  -0.0000  |  0.0022|\n",
    "|   -0.0035 |  -0.0044  |  1.0000|\n",
    "\n",
    "Where **E** is the essential matrix composed of information of the Transpose and Rotation, the extrinsic parameters. **F** encodes both intrinsic and extrinsic parameters.\n",
    "\n",
    "![correspondence](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/matching.png?raw=true)\n",
    "\n",
    "#### Epipoles ####\n",
    "|   Left     |   Right |\n",
    "| :---:      |    :----:   | \n",
    "| -1.3191919e+02|  -1.1593362e+04|\n",
    "| 3.3683784e+02|  -1.2446050e+03|\n",
    "| 1.0000000e+00|  1.0000000e+00|\n",
    "  \n",
    "A triangulation system to align both right and left cameras from the images.\n",
    "\n",
    "![accuracy](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/accuracy.png?raw=true)\n",
    "\n",
    "#### Accuracy ####\n",
    "We calculate the accuracy using our correspondance pairs. (Using too many points make the system unstable.)\n",
    "\n",
    "|   Left     |   Right |  err (difference) |\n",
    "| :---:      |    :----:   | :----:|\n",
    " | 215.8950|  141.6493| 74.2458|\n",
    "  | 211.4315|  122.7491| 88.6824|\n",
    "  | 101.2824|  24.2931|76.9893|\n",
    "  |26.2710|26.2779| 0.0069|\n",
    "\n",
    "#### Bonus automated point selection: \n",
    "\n",
    "Asssumption: *constant Intensity* when image taken from time *t* (left) to time *t+1*  (right).\n",
    "\n",
    "Apply a 3x3 convolution matrix (Sobel Vertical multiplier)\n",
    "\n",
    "|column 1 | column 2  | column 3|\n",
    "| :---:      |    :----:   | :----:|\n",
    " | -1  |  0|1|\n",
    " | -2  |  0|2|\n",
    " | -1  |  0|1|\n",
    "  \n",
    "(Sobel Horizontal multiplier is the transpose of the Sobel Vertical multiplier)\n",
    "Using the top 5th percentile of your normalized intensity after doing edge detection we match these points in pairs between the left and right images. We pair the left and right pixels by noting their coordinates sequentially. Taking these points for matching our accuracy is the following:\n",
    "\n",
    "|left |right | diff|\n",
    "| :---:      |    :----:   | :----:|\n",
    " | a<sub>l</sub>|  a<sub>r</sub>| abs(a<sub>l</sub>-a<sub>r</sub>)|\n",
    " | b<sub>l</sub>|  b<sub>r</sub>| abs(b<sub>l</sub>-b<sub>r</sub>)|\n",
    " | c<sub>l</sub>|  c<sub>r</sub>| abs(c<sub>l</sub>-c<sub>r</sub>)|\n",
    " | d<sub>l</sub>|  d<sub>r</sub>| abs(d<sub>l</sub>-c<sub>r</sub>)|\n",
    "\n",
    "For precision apply a combination of gradient mapping to categories colors or Hough transformation to remove imperfections then filter results for your edge mapping.\n",
    "\n",
    "## [Feature Based Matching](https://www.mathworks.com/help/vision/ref/epipolarline.html)\n",
    "\n",
    "For feature based matching we will use a correlation curve. Consider the equation:\n",
    "\n",
    "![Correlation](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/Correlation.png?raw=true)\n",
    "\n",
    "Where you are correlating the disparity:\n",
    "\n",
    "![Disparity](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/Disparity.png?raw=true)\n",
    "\n",
    "For the correlation approach you can never get zero, so that means for the cross-correlation approach disparity is inherent: this is practical.\n",
    "\n",
    "![Similarity_Criterion](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/Similarity_Criterion.png?raw=true)\n",
    "\n",
    "For sum of square difference and sum of absolute different you can achieve 0 and that would be your max value. They produce similar curves to cross-correlation (a parobola).\n",
    "\n",
    "An advantage to using a correlation approach is that is allows you to use special non-rectangular shapes for your feature domain for the user to select points in optimizing correspondence. In applying feature based matching by limiting our correspondence domain to epipolar lines we obtain the following accuracy for given fundamental matrix:\n",
    "\n",
    "**<center>Feature Based Fundamental Matrix</center>**\n",
    "\n",
    "|column 1 | column 2  | column 3|\n",
    "------------|---------|---------|\n",
    "|0.0000 |  -0.0004  | 0.0348|\n",
    "|  0.0004 |  0.0000  |  -0.0937|\n",
    "|  -0.0425 |   0.0993   |    0.9892|\n",
    "\n",
    "**<center>Feature Based accuracy</center>**\n",
    "\n",
    "|   Left     |   Right |  err (difference) |\n",
    "| :---:      |    :----:   | :----:|\n",
    " | 480.5000|  0.5000 | 480|\n",
    "  | 480.5000 |  480.5000| 0|\n",
    "  | 0.5000|  0.5000|0|\n",
    "  |480.5000|480.5000| 0|\n",
    "  \n",
    "  \n",
    "## Discussion\n",
    "         \n",
    "![Feature1](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/FeatureBasedMatching-left.png?raw=true)\n",
    "![Feature2](https://github.com/adomakor412/ComputerVision/blob/master/VisualMotion/FeatureBasedMatching-right.png?raw=true)\n",
    "              \n",
    "As we can see from feature based matching, most of the points are highly accurate, however, a misinterpretation can lead to a huge discrepancy for a single point. For user based matching we saw that the error was consistent from point to point. Globally, in this example we see that feature based matching has a and user based matching have similar variance in error, where user based matching has a lower average in error.\n",
    "      \n",
    "Notice the points chosen in feature based matching utilized gradient mapping where points chosen were mostly centered on abrupt changes in color (from a building to grass land) as a characteristic of detecting edges. This characteristic is accurate for detecting singular points in non-patterned images. This is a favorable apporach from an automatation point of view.\n",
    "\n",
    "The user based matching tend to seek corners of objects which are more easily identifiable by the human eye. In accuracies here are from hand to mouse click coordination and limitation of the human eye to see closely to a single pixel. Points are chosen in region of precision and the error reflects that. In addition, points tend to be chosen in clusters, which can keep the absolute value epipole errors from image to image small by limiting the divergence of the epipolar line extrapolation. This accounts for a suite of features or characteristics such as occlusion, texture, smoothness including edges, corners, and color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
