{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1   (Camera Models- 20 points)  Prove that the vector from the viewpoint of a pinhole camera to the vanishing point (which is a point on the imageplane) of a set of 3D parallel lines in space is parallel to the direction of that set parallel lines. Please show steps of your proof.\n",
    "\n",
    "Hint: You can either use geometric reasoning or algebraic calculation. \n",
    "If you choose to use geometric reasoning, you can use the fact that the projection of a 3D line in space is the intersection of its “interpretation\n",
    "plane” with the image plane.  Here the interpretation plane (IP) is a plane passing through the 3D line and the center of projection (viewpoint) of the\n",
    "camera.  Also, the interpretation planes of two parallel lines intersect in a line passing through the viewpoint, and the intersection line is parallel to\n",
    "the parallel lines.\n",
    "If you select to use algebraic calculation, you may use the parametric representation of a 3D line: P = P0 +tV, where P= (X,Y,Z)  is any point on the\n",
    "line (here    denote for transpose),   P0 = (X0,Y0,Z0)  is a given xed point on the line, vector V = (a,b,c)  represents the direction of the line, and t is\n",
    "the scalar parameter that controls the distance (with sign) between P and P0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SOLUTION 1](http://www.cs.toronto.edu/~jepson/csc420/notes/imageProjection.pdf)\n",
    "Let $$\\textbf{P}_w = \\textbf{P_0} + t\\textbf{V}$$ \n",
    "\n",
    "Where **P** is a vector representing a point from the image plane to a vanishing point in 3D-space. The vector **V** represent a line to point *P(a,b,c)* that is in the direction to the vanishing point.\n",
    "\n",
    "$$\\begin{bmatrix}a \\\\ b \\\\ c\\end{bmatrix}$$ \n",
    "\n",
    "\n",
    "\n",
    "All lines parallel to **V** converge as the scalar value *t* goes to infinity i.e.\n",
    "\n",
    "$$ \\lim_{t \\to \\infty} \\textbf{P}_w = \\textbf{V} $$ \n",
    "\n",
    "for any parallel point in 3D space: for it shares the same vector V for corresponding starting point **P<sub>w,image</sub>** in the image plane.\n",
    "\n",
    "For **P** in the perspective projection space we have the formulation **P= R•P<sub>w</sub>+T**, where **T<sub>3x1</sub>** is constant. Note **R<sub>3x3</sub>** can be found from $[\\bar{vp_{1,1x3}}, \\bar{vp_{2,1x3}}, \\bar{vp_{3,1x3}}]^T$ where $\\bar{vp_i}$ are normalized vanish points formed from three sets of parallel lines mutually orthogonal to one another.\n",
    "\n",
    "Let $P_{w}'= P+\\Delta P_0 + t V + \\Delta V$, be a parallel line\n",
    "\n",
    "$V= [a,b,c]^T$, where $tV$ forms a vanish point in perspective space\n",
    "\n",
    "$\\Delta P_{0;i,j} || \\Delta V_{0;i,j}$, form a  parallel line\n",
    "\n",
    "$P_{w}'= P + tV + (\\Delta P_0 + \\Delta V)$\n",
    "\n",
    "Then $P' = R[(P_0 + t V) + (\\Delta P_0 + \\Delta V)] + T$, where $(\\Delta P_0 + \\Delta V) \\equiv \\Delta P_w'$\n",
    "\n",
    "$P' = R[(P_0 + t V)] + R\\Delta P_w' + T$, where $R • \\Delta P_w' \\equiv \\Delta T$\n",
    "\n",
    "$P' = R[(P_0 + t V)] + \\Delta T + T$\n",
    "\n",
    "$\\textbf{P'} = \\textbf{P} + \\Delta \\textbf{T} $ **(Eq 1)**\n",
    "\n",
    "The line formed from the pinhole to the vanish point in world coordinates can be represented by the following:\n",
    "\n",
    "$\\Delta V - P'$, using vector notation: head minus tail\n",
    "\n",
    "$\\Delta V - P' = \\Delta V  - (P +\\Delta T) = \\Delta V  - (P + R(\\Delta P_0 + \\Delta V))$\n",
    "\n",
    "Recall, $R$ can be formed by the normalized vanish points therefore its cross product with $\\Delta V$ turns one its rows to zero, \n",
    "\n",
    "similarly $\\Delta P_0$ is parallel to $\\Delta V$, so $R$ cross $\\Delta P_0$ turn that $R$ row to zero\n",
    "\n",
    "We have $\\Delta V - P' = \\Delta V  - P - R (\\Delta P_0 + \\Delta V)$, interpreting matrix orientation we have\n",
    "\n",
    "$\\Delta V - P' = \\Delta V  - P - (R - [0, 0, f]^T)$, and $(R - [0, 0, f]^T)$ is a translation of between the image plane and focal plane. Droping the $\\Delta V$ from both sides, by definition we have:\n",
    "\n",
    "$ \\textbf{- P'} =  \\textbf{-P} - \\textbf{P}_\\textbf{w}•\\textbf{T}$, **substituting** (Eq 1) we get \n",
    "\n",
    "$\\Delta T = P_w•T$, where we know $\\Delta T = R • \\Delta P_w'$ and $\\Delta P_w' || P$\n",
    "\n",
    "$R • \\Delta P_w' = P_w•T$ We have individual components on either side of the matrices parallel as a property\n",
    "\n",
    "\n",
    "... the 3D line formed from the pinhole to a vanish point in the perspective plane is parallel to the set of 3D lines which forms that vanish point. \n",
    "\n",
    "\n",
    "\n",
    "### [SOLUTION 2](https://en.wikipedia.org/wiki/Vanishing_point)\n",
    "\n",
    "![geo](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/Geometric.png?raw=true)\n",
    "\n",
    "1. A set of 3D parallel lines form respect lines in the perspective plane.\n",
    "2. Those lines in the perspective plane intersect forming a vanish point.\n",
    "3. The 3D line projected from the pinhole to the vanish point is parallel to the set of parallel lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. (Camera Models- 20 points) Show that relation between any image point (xim, yim)   (in the form of (x1,x2,x3)  in projective space ) of a planar surface in 3D space and its corresponding point (Xw, Yw, Zw)  on the plane in 3D space can be represented by a 3×3 matrix. \n",
    "\n",
    "You should start from the general form of the camera model (x1,x2,x3)  = M M (Xw, Yw, Zw, 1) , where the image center (ox, oy), the focal length f, the scaling factors(\n",
    "sx and sy),  the rotation matrix R and the translation vector T are all unknown. Note that in the course slides and the lecture notes, I used a\n",
    "simplied model of the perspective project by assuming ox and oy are known and sx = sy =1, and only discussed the special cases of a plane. So you\n",
    "cannot directly copy those equations I used.  Instead you should use the general form of the projective matrix, and the  general form of a plane\n",
    "n  X  + n  Y  + n  Z   = d. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SOLUTION](https://www.cis.upenn.edu/~jean/gma-v2-chap5.pdf)\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "           x_{im} \\\\\n",
    "           y_{im}\n",
    "         \\end{bmatrix}\n",
    "         =\n",
    "         \\begin{bmatrix}\n",
    "           u_{i}/w_{i} \\\\\n",
    "           v_{i}/w_{i}\n",
    "         \\end{bmatrix}\n",
    "         <=\n",
    "         \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           x_{3}\n",
    "         \\end{bmatrix}\n",
    "         =\n",
    "         \\begin{bmatrix}\n",
    "           u_{i} \\\\\n",
    "           v_{i} \\\\\n",
    "           w_{i}\n",
    "         \\end{bmatrix}\n",
    "         =\n",
    "         \\textbf{M} \n",
    "         \\begin{bmatrix}\n",
    "           X_{i} \\\\\n",
    "           Y_{i} \\\\\n",
    "           Z_{i} \\\\\n",
    "           1\n",
    "         \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$(x_{im},y_{im}) \\text{ is the image point in image (frame) coordinates derived from the perspective projection matrix: } (u_{i},v_{i},w_{i}).$$\n",
    "\n",
    "\n",
    "$$(X_{i},Y_{i},Z_{i}) \\text{ is the object in world coordinates.}$$\n",
    "\n",
    "\n",
    "Notice, in the world coordinates there is the projective space of the object. Notice there is a 1 appended to the 3D world vector. This is to account for any shift in the apparatus when converting to the image. The shifts are absorbed in the projective matrix: **M = M<sub>int</sub> * M<sub>ext</sub>**.\n",
    "\n",
    "The intrinsic parameters accounts for the principal point *(o<sub>x</sub>,o<sub>y</sub>)* and scaling factors *(s<sub>x</sub>,s<sub>y</sub>)* and noted as so:\n",
    "\n",
    "**M<sub>int</sub>**=$$\\begin{bmatrix}-f_{i} & 0 & o_{x} \\\\0 & -f_{i} & o_{y} \\\\0 & 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "The extrinsic paramaters account for rotations and tranpositions:\n",
    "\n",
    "**M<sub>ext</sub>**=$$\\begin{bmatrix}r_{11} & r_{12} & r_{13} & T_{x} \\\\r_{21} & r_{22} & r_{23} & T_{y} \\\\r_{31} & r_{32} & r_{33} & T_{z} \\end{bmatrix}$$\n",
    "\n",
    "and all together:\n",
    "\n",
    "**M**=\n",
    "$$\\begin{bmatrix}-f_{x}r_{11}+o_{x}r_{31}&-f_{x}r_{12}+o_{x}r_{32}&-f_{x}r_{13}+o_{x}r_{33}&-f_{x}T_{x}+o_{x}T_{z} \\\\-f_{y}r_{21}+o_{y}r_{31}&-f_{y}r_{22}+o_{y}r_{32}&-f_{y}r_{23}+o_{x}r_{33}&-f_{y}T_{y}+o_{y}T_{z} \n",
    "\\\\r_{31}&r_{32}&r_{33}&T_{z}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Lastly, recalling alignment from the optical axis anaology in the camera model, we implement a scale factor\n",
    "\n",
    "$$\\gamma = \\begin{vmatrix}R_{3}^{T}\\end{vmatrix}$$ such that\n",
    "\n",
    "$$\\hat{\\textbf{M}}= \\gamma \\textbf{M}$$\n",
    "\n",
    "From our plane equation we also have \n",
    "\n",
    "$$n_x X_w + n_y Y_w + n_z Z_w = d$$\n",
    "\n",
    "Define $M$ such that $[X, Y, Z]^T = M•[X_w, Y_w, Z_w]^T$. Consider the fourth column of $M$, we have:\n",
    "\n",
    "$$\\hat{M}$$ =\n",
    "$$\\begin{bmatrix}-f_{x}r_{11}+o_{x}r_{31}&-f_{x}r_{12}+o_{x}r_{32}&-f_{x}r_{13}+o_{x}r_{33} \\\\-f_{y}r_{21}+o_{y}r_{31}&-f_{y}r_{22}+o_{y}r_{32}&-f_{y}r_{23}+o_{x}r_{33}\n",
    "\\\\r_{31}&r_{32}&r_{33}\n",
    "\\end{bmatrix} + \\gamma \\begin{bmatrix}\n",
    "           n_x(-f_{x}T_{x}+o_{x}T_{z}) \\\\\n",
    "           n_y(-f_{y}T_{y}+o_{y}T_{z}) \\\\\n",
    "           n_z(T_{z})\n",
    "         \\end{bmatrix} \\begin{bmatrix}\n",
    "           1/x_{1} \\\\\n",
    "           1/x_{2} \\\\\n",
    "           1/x_{3}\n",
    "         \\end{bmatrix}$$\n",
    "         \n",
    "\n",
    "Where $$ d =\\begin{bmatrix}\n",
    "           n_{x} \\\\\n",
    "           n_{y} \\\\\n",
    "           n_{z}\n",
    "         \\end{bmatrix}\\begin{bmatrix}\n",
    "           1/x_{1} \\\\\n",
    "           1/x_{2} \\\\\n",
    "           1/x_{3}\n",
    "         \\end{bmatrix}$$\n",
    "         \n",
    "Therefore $$\\gamma = 1/d$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  (Calibration- 20 points )  Prove the Orthocenter Theorem by geometric arguments: Let T be the triangle on the image plane dened by the threevanishing points of three mutually orthogonal sets of parallel lines in space. Then the image center is the orthocenter of the triangle T \n",
    "(i.e., the\n",
    "common intersection of the three altitudes.  Note that you are asked to prove the Orthocenter Theorem rather than that the orthocenter itself as\n",
    "the common interaction of the three altitudes, which you can use as a fact. \n",
    "(1)    Basic proof: use the result of Question 1, assuming the aspect ratio of the camera is 1. (10 points)\n",
    "(2)    If you do not know the  focal length of the camera, can you still nd the image center using the Orthocenter Theorem? Can you further\n",
    "estimate the focal length? For both questions, please show why (and then how) or why not. (5 points)\n",
    "(3)    If you do not know the aspect ratio of the camera, can you still nd the image center using the Orthocenter Theorem? **Show why or why\n",
    "not**. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SOLUTION](https://www.youtube.com/watch?v=HSGM5o_WWlc)\n",
    "\n",
    "For the Orthocenter theorem we use a logical proof. (It is an if and only if proof.)\n",
    "\n",
    "![pinholeCM](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/pinoleCM.png?raw=true)\n",
    "\n",
    "Recall from the pinhole camera model that the image center is along the optical axis. We see that the focal point for all parallel points on an object plane will have the same Field of View (FOV) (angles) in the perspective projection. Consider the equivalent mathematical model where the image plane is moved to the focal plane. (Notice that the image center and the focal node are now one in the same.)\n",
    "\n",
    "![EquivalentGeometry](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/EquivalentGeometry.png?raw=true)\n",
    "\n",
    "The property we see for the pinhole camera model is that for a set of three mutual parallel lines, the [ProspectiveTransformation](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/ProspectiveTransformation.png?raw=true) has parallel lines intersect forming vanishing points. \n",
    "\n",
    "![ProspectiveTransformation](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/ProspectiveTransformation.png?raw=true)\n",
    "\n",
    "The FOV for these vanishing points are taken at infinity therefore the angle is 90° taken at the limit. Note that these vanishing points are outside the image view.\n",
    "\n",
    "![FocalLengthFOV](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/FocalLengthFOV.png?raw=true)\n",
    "\n",
    "The physical length of these vanishing points form lines of altitude to the optical axis(AH, CH, BH). Form a triangle from these three vanishing points (triangle ABC). Form a circle from these three vanishing points (circle ABC [not drawn]). Locate the corresponding circumcenter (O) of this circle on the focal plane. This corresponds to the circumcenter on the image plane. Connect the circumcenter to the intersection point formed from lines of altitude (OH). \n",
    "\n",
    "Since the image center is defined as the point that corresponds to the connection of the focal plane to the image plane on the optical axis, the image center is the orthocenter.\n",
    "\n",
    "#### Estimating orthocenter\n",
    "\n",
    "You don't need to know the focal length to obtain image center. You don't need to solve for 8 points from the matrix theorem:\n",
    "![ProjectionMatrixMethod](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/ProjectionMatrixMethod.png?raw=true)\n",
    "Instead, you can use the orthocenter theorem to estimate the image center.\n",
    "\n",
    "#### Finding the aspect ratio and estimating the focal length.\n",
    "\n",
    "Without the aspect ratio you are unable to find the focal length because you're unable to map between the extrinsic and intrinsic values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calibration Programming Exercises (40 points): Implement the direct parameter calibration method in order to \n",
    "(1) learn how to use SVD to solve systems of linear equations; \n",
    "\n",
    "(2) understand the physical constraints of the camera parameters; and \n",
    "\n",
    "(3) understand important issues related to\n",
    "calibration, such as calibration pattern design, point localization accuracy and robustness of the algorithms. Since calibrating a real camera involves\n",
    "lots of work in calibration pattern design, image processing and error controls as well as solving the equations, we will mainly use simulated data to\n",
    "understand the algorithms.  As a by-product we will also learn how to generate 2D images from 3D models using a “virtual” pinhole camera. The\n",
    "calibration procedure has the following three steps:\n",
    "\n",
    "**1.** Calibration pattern “design”. Generate data of a “virtual” 3D cube similar to the one shown in here of the lecture notes in camera calibration. For\n",
    "example, you can hypothesize a 1x1x1 m  cube and pick up the coordinates of 3-D points on corners of each black square in your world coordinate\n",
    "system. Make sure that your data is sucient for the following calibration procedures. In order to check the correctness of your data, draw your\n",
    "cube (with the control points marked) using Matlab (or whatever tools you are selecting). I have provided a piece of starting code in Matlab for you\n",
    "to use.\n",
    "\n",
    "**2.** A “virtual” camera and its images. Design a “virtual” camera with known intrinsic parameters including focal length f, image center (o , o ) and\n",
    "pixel size (s , s ).  As an example, you can assume that the focal length is f = 16 mm, the image frame size is 512*512 (pixels) with (o ,o ) = (256, 256),\n",
    "and the size of the image sensor  inside your camera is 8.8 mm *6.6 mm (so the pixel size is (s ,s ) = (8.8/512, 6.6/512) ). Capture an image of your\n",
    "“virtual” calibration cube with your virtual camera in a given pose (i.e., R and T).  For example, you can take the picture of the cube 4 meters away\n",
    "and with a tilt angle of 30 degree. Use three rotation angles alpha, beta, gamma to generate the rotation matrix R (refer to the lecture notes in\n",
    "camera model – please check the correctness of the R equation especially for signs).  You may need to try dierent pose in order to have a suitable\n",
    "image of your calibration target. In your report, please clearly list the parameters you used for generating your calibration image. They are called\n",
    "the “ground truth” of the parameters.\n",
    "\n",
    "**3.** Direction calibration method: Estimate the intrinsic (f , f , aspect ratio a, image center (o ,o ) ) and extrinsic (R, T and further alpha, beta, gamma)\n",
    "parameters. Use SVD to solve the homogeneous linear system and the least square problem, and to enforce the orthogonality constraint on the\n",
    "estimate of R.  You are asked to do the following:\n",
    "        \n",
    "**i.**      Use the accurately simulated data (both 3D world coordinates and 2D image coordinates) to the algorithms, and compare the results with\n",
    "the “ground truth” data (which are given in step (a) and step (b)).  Remember you are practicing a camera calibration, so you should pretend you\n",
    "know nothing about the camera parameters (i.e. you cannot use the ground truth data in your calibration process). However, in the direct\n",
    "calibration method, you could use the knowledge of the image center (in the homogeneous system to nd extrinsic parameters) and the aspect\n",
    "ratio (in the Orthocenter theorem method to nd image center). \n",
    "\n",
    "**ii.**      Show experimentally whether the unknown aspect ratio matters in estimating the image center, and how the initial estimation of image\n",
    "center aects the estimating of the remaining parameters.  Give a solution to solve the problems if any and implement it.\n",
    "\n",
    "**iii.**      Accuracy Issues. Add in some random noises to the simulated data and run the calibration algorithms again. See how the “design tolerance”\n",
    "of the calibration target and the localization errors of 2D image points aect the calibration accuracy. For example, you can add 0.5 mm random\n",
    "error to 3D points and 1.0 pixel random error to 2D points. Also analyze how sensitive of the Orthocenter method is to the extrinsic parameters in\n",
    "imaging the three sets of the orthogonal parallel lines. (* extra points:10)\n",
    "In all of the steps, you should give you results using either tables or graphs, or both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "![CalibrationPattern](http://ccvcl.org/wp-content/uploads/2019/10/calibrationpattern.gif)\n",
    "\n",
    "\n",
    "#### [Ground truth parameters](https://apple.stackexchange.com/questions/267746/what-is-the-focal-length-of-the-iphone-7s-camera-when-accounting-for-video-crop)\n",
    "focal length is **16** mm.\n",
    "\n",
    "<ins>image frame size</ins> is **512x512** (pixelation) with (o<sub>x</sub>,o<sub>y</sub>) = **(256, 256)**,\n",
    "\n",
    "image sensor inside camera is **8.8 mm. x 6.6 mm.** (so the pixel size is (s<sub>x</sub>,s<sub>y</sub>) = (0.0172 mm per pixel, 0.13 mm per pixel) ).\n",
    "\n",
    "#### [Direct Calibration Method](https://www.pyimagesearch.com/2015/01/19/find-distance-camera-objectmarker-using-python-opencv/)\n",
    "\n",
    "Estimate the intrinsic parametes (f<sub>x</sub> , f<sub>y</sub>, aspect ratio a, image center (o<sub>x</sub> ,o<sub>y</sub>) )\n",
    "\n",
    "extrinsic parameters (R, T and further alpha, beta, gamma). \n",
    "\n",
    "Use SVD to solve the homogeneous linear system and the least square problem, and to enforce the orthogonality constraint on the estimate of R. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the chosen points A we solve [U,S,V] = svd(A) to obtain V:\n",
    "\n",
    "**A =**\n",
    "\n",
    "#### ![A](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/A.png?raw=true)\n",
    "\n",
    "**V =**\n",
    "#### ![V](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/V.png?raw=true)\n",
    "\n",
    "**R =**\n",
    "#### ![V](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/V.png?raw=true)\n",
    "\n",
    "**T =**\n",
    "#### ![V](https://github.com/adomakor412/ComputerVision/blob/master/CameraCalibration/images/V.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  9  10 196 ... 160 143  88]\n",
      "  [109 131 231 ...  23   2  13]\n",
      "  [114 183 164 ...  59 188 248]\n",
      "  ...\n",
      "  [ 13 111 194 ... 191  98  87]\n",
      "  [ 29 167 163 ... 223  37 244]\n",
      "  [ 98 223  99 ... 113  50  69]]\n",
      "\n",
      " [[103 167  98 ... 192 115 252]\n",
      "  [230  78  15 ...  36 216 143]\n",
      "  [104  92 186 ... 200  72  14]\n",
      "  ...\n",
      "  [ 23 177  23 ...  17 133  49]\n",
      "  [249  23 190 ... 149 237 164]\n",
      "  [128 220  71 ...  16  50  21]]\n",
      "\n",
      " [[ 80 234 180 ... 233  53  10]\n",
      "  [ 18 100   6 ...  66 233  29]\n",
      "  [ 61 217 140 ...  26  18 250]\n",
      "  ...\n",
      "  [ 92 199  63 ... 133 142  43]\n",
      "  [251 211 218 ... 185 129 198]\n",
      "  [251  29  45 ... 228  52 118]]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'figure' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c38cfa69f9fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# plot projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'figure' is not defined"
     ]
    }
   ],
   "source": [
    "import camera\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "file = \"images/QueensGambit.jpeg\"\n",
    "image = cv.imread(file)\n",
    "\n",
    "# load points\n",
    "shape = np.shape(image)\n",
    "P = np.array(image)#.T\n",
    "P_prime = P.reshape(shape[2],shape[0],shape[1])\n",
    "P_prime_CT = P.reshape(shape[2],shape[1],shape[0])\n",
    "#points = np.vstack((points,np.ones(points.shape[1])))\n",
    "\n",
    "# setup camera\n",
    "print(np.matmul(P_prime,P_prime_CT))\n",
    "# print(np.dot(P,P_prime))\n",
    "\n",
    "\n",
    "# cam = camera.Camera(P)\n",
    "# x = cam.project(points)\n",
    "\n",
    "# plot projection\n",
    "plt.figure()\n",
    "plt.plot(x[0],x[1],'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-4jcifzim/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-134e843f03da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mDirectCalibration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mKNOWN_DISTANCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8.5\u001b[0m \u001b[0;31m#inches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/COURSES/ComputerVision/CameraCalibration/DirectCalibration.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# the focal length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"QueensGambit.jpeg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mmarker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_marker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mfocalLength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mKNOWN_DISTANCE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mKNOWN_WIDTH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/COURSES/ComputerVision/CameraCalibration/DirectCalibration.py\u001b[0m in \u001b[0;36mfind_marker\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_marker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# convert the image to grayscale, blur it, and detect edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0medged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCanny\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m125\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.4.0) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-4jcifzim/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import DirectCalibration as DC\n",
    "\n",
    "\n",
    "KNOWN_DISTANCE = 8.5 #inches\n",
    "\n",
    "KNOWN_WIDTH = 4032*1.42 #inches\n",
    "'''\n",
    "load the first image that contains an object that is KNOWN TO BE 1 feet\n",
    "from our camera, then find the paper marker in the image, and initialize\n",
    "the focal length\n",
    "'''\n",
    "\n",
    "print(len(image))\n",
    "\n",
    "f = DC.findFocalLength(KNOWN_DISTANCE,KNOWN_WIDTH,image)\n",
    "print(f)\n",
    "print(f/(1.89))\n",
    "print(f/(1.42))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Factoring The Camera Matrix](https://culturalengineerassociation.weebly.com/uploads/8/6/7/7/86776910/programming_computer_vision_with_python.pdf)\n",
    "\n",
    "The estimation is close to ground truth. The estimated focal length is 0.39 inches, 92.3% accuracy with  f_x,e = 0.21 and f_y,e = 0.28. The estimated aspect ratio is 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "# termination criteria\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*7,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "images = glob.glob('images/*')\n",
    "#print(len(images))\n",
    "for fname in images:\n",
    "    img = cv.imread(fname)\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv.findChessboardCorners(gray, (7,6), None)\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        corners2 = cv.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria)\n",
    "        imgpoints.append(corners)\n",
    "        # Draw and display the corners\n",
    "        cv.drawChessboardCorners(img, (7,6), corners2, ret)\n",
    "        cv.imshow('img', img)\n",
    "        \n",
    "        cv.waitKey(500)\n",
    "        print(cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None))\n",
    "        \n",
    "        print('hello')\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-47d143a398ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import scipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalibrateCamera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gray' is not defined"
     ]
    }
   ],
   "source": [
    "#import scipy\n",
    "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
